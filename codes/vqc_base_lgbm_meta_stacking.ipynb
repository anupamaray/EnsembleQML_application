{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Stacking with VQC as base classifier and LGBM as meta classifier**\n",
        "In this tutorial, we will see the classical stacking of VQC and LGBM algorithms, with VQC as base classifier and LGBM as meta classifier. The stacking algorithm works as below\\\n",
        "1) The base classifiers are trained with training data\\\n",
        "2) The trained base classifiers are used to test both training data and testing data\\\n",
        "3) The output labels from base classifiers on training and testing data are appended as features to original training and testing data\\\n",
        "4) Now we train the meta classifier with appended training data and test it on appended testing data to get final prediction values"
      ],
      "metadata": {
        "id": "Y-YlvvMOewHA"
      },
      "id": "Y-YlvvMOewHA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is detailed explanation and implementation of each of above steps"
      ],
      "metadata": {
        "id": "2uCfFcjQQ8VU"
      },
      "id": "2uCfFcjQQ8VU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDRv2RL3fF42"
      },
      "source": [
        "We first need to install qiskit and qiskit_machine_learning modules if not already installed. We start by importing all necessary libraries from qiskit,  qiskit\\_machine\\_learning and some related modules as shown in below cells."
      ],
      "id": "CDRv2RL3fF42"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba5f1a2c",
      "metadata": {
        "id": "ba5f1a2c"
      },
      "outputs": [],
      "source": [
        "from qiskit import BasicAer\n",
        "from qiskit.utils import QuantumInstance, algorithm_globals\n",
        "from qiskit.algorithms.optimizers import COBYLA\n",
        "from qiskit.circuit.library import TwoLocal\n",
        "from qiskit_machine_learning.algorithms import VQC\n",
        "from qiskit_machine_learning.circuit.library import RawFeatureVector\n",
        "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap, ZFeatureMap, NLocal\n",
        "from qiskit import QuantumCircuit\n",
        "from qiskit.circuit import Parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a0ab1a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a0ab1a5",
        "outputId": "6fb738dd-5a25-4151-cab4-98090d27aa4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '/root/.ipython']\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.path)\n",
        "sys.path.append(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5f149ce",
      "metadata": {
        "id": "c5f149ce"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import svm, metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from time import time\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data set\n",
        "We use ethereum networks data to perform stacking. We consider 7 features for data set\\\n",
        "1) Degree - In degree, Out degree, Total degree\\\n",
        "2) Strength - In strength, Out strength, Total strength\\\n",
        "3) Number of neighbours\n",
        "\n",
        "The files 'train_data.npy' and 'train_labels.npy' have 960 training data points and corresponding labels respectively, first 160 training data points are of phishing nodes and next 800 are of non-phishing nodes. The files 'test_data.npy' and 'test_labels.npy' have 11000 testing data points and corresponding labels respectively, first 1000 testing data points are of phishing nodes and next 10000 are of non-phishing nodes.\n",
        "\n",
        "\n",
        "let us choose 320 training data points to perform stacking, 160 phishing and 160 non-phishing. Even though the real life data is highly imbalanced, we choose equal number of phishing and non-phishing nodes for training our classifiers and then test on imbalanced testing data. With different experiments on this data set, taking balanced training data proved more effective in giving good results"
      ],
      "metadata": {
        "id": "92iLRTqqR72a"
      },
      "id": "92iLRTqqR72a"
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.load('train_data.npy')\n",
        "train_labels = np.load('train_labels.npy')\n",
        "test_data = np.load('test_data.npy')\n",
        "test_labels = np.load('test_labels.npy')\n",
        "train_data = train_data[:320]\n",
        "train_labels = train_labels[:320]"
      ],
      "metadata": {
        "id": "-yBzEhmheg4X"
      },
      "id": "-yBzEhmheg4X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAMqYeEyflh9"
      },
      "source": [
        "# Classical SVM training and testing\n",
        "We perform classical support vector machine classification to compare our final results"
      ],
      "id": "qAMqYeEyflh9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e4eba1",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3e4eba1",
        "outputId": "f672b610-4ef7-4161-9dac-7ad56a59c8e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Training a classical SVM classifier with rbf Kernel ***\n",
            "training time for classical SVM :  0.006948947906494141\n",
            "[[9676  324]\n",
            " [ 559  441]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.97      0.96     10000\n",
            "           1       0.58      0.44      0.50      1000\n",
            "\n",
            "    accuracy                           0.92     11000\n",
            "   macro avg       0.76      0.70      0.73     11000\n",
            "weighted avg       0.91      0.92      0.91     11000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#training classical SVM with rbf kernel\n",
        "\n",
        "print(\"*** Training a classical SVM classifier with rbf Kernel ***\")\n",
        "\n",
        "#converting two dimensional labels to 1D\n",
        "train_labels_svm = train_labels[:,0]\n",
        "test_labels_svm = test_labels[:,0]\n",
        "\n",
        "clf = svm.SVC(kernel='rbf')\n",
        "start_time = time()\n",
        "clf.fit(train_data, train_labels_svm)\n",
        "end_time = time()\n",
        "duration = end_time - start_time\n",
        "print(\"training time for classical SVM : \", duration)\n",
        "y_pred=clf.predict(test_data)\n",
        "print(confusion_matrix(test_labels_svm, y_pred))\n",
        "print(classification_report(test_labels_svm, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "779169f9"
      },
      "source": [
        "# **Stacking**\n",
        "###Initializing the base and meta classifiers\n",
        "Initialize the base classifier VQC. ZZ feature map is used to encode classical data into quantum for VQC. it can be described using below equations\\\n",
        "$ZZ feature map = U_{\\phi(x)}H^{\\otimes m}$\\\n",
        "where $U_{\\phi(x)} = exp\\left(j\\sum_{k\\in [m]}\\phi_k(x_i)\\prod_{l\\in k}Z_l\\right)$, \n",
        "$[m]=\\{1,\\dots,m,(1,2),(1,3),\\dots,(m-1,m)\\}$,\\\n",
        "where $\\phi_p(x_i) = x_i^{(p)}$ and $\\phi_{(p,q)}(x_i)=(\\pi-x_i^{(p)})(\\pi-x_i^{(q)})$\\\n",
        "$Z=pauli-Z$ $gate$, $j = imaginary$ $unit$\n",
        "\n",
        "We use TwoLocal ansatz(parametrized quantum circuit with trainable parameters) for this classifier. Two local ansatz is a parameterized circuit consisting of alternating rotation layers and entanglement layers. The rotation layers are single qubit gates applied on all qubits. The entanglement layer uses two-qubit gates to entangle the qubits according to a strategy set using entanglement. For more information about two local ansatz refer [Two local ansatz reference](https://qiskit.org/documentation/stubs/qiskit.circuit.library.TwoLocal.html#:~:text=The%20two-local%20circuit%20is%20a%20parameterized%20circuit%20consisting,rotation%20and%20entanglement%20gates%20can%20be%20specified%20as)\n",
        "\n",
        "Multiple ansatzes can be used in one VQC, however after experimentation with different number of repetitions of ansatzes, we identified that for this data set shallow circuit is more suitable, thus we are using only two repetitions of ansatz in this tutorial and not going beyond it. For reference we can look at figure 10 in below paper\n",
        "\n",
        "VQC is trained with maxiter 100 and COBYLA is chosen as our optimizer to update its parameters. \n",
        "\n",
        "Statevector simulator is used to simulate the results of quantum computer, it can be replaced with backend for harware results\n",
        "\n",
        "Initialize LGBM meta classifier"
      ],
      "id": "779169f9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79a6dd54",
      "metadata": {
        "id": "79a6dd54"
      },
      "outputs": [],
      "source": [
        "seed = 1376\n",
        "\n",
        "#feature dimensions\n",
        "feature_dim = train_data.shape[1]\n",
        "\n",
        "#feature map of VQC\n",
        "feature_map = ZZFeatureMap(feature_dim)\n",
        "\n",
        "#ansatz we use in VQC\n",
        "ansatz = TwoLocal(feature_dim, ['ry', 'rz'], 'cz', reps = 2)\n",
        "\n",
        "#initialize VQC\n",
        "vqc = VQC(feature_map=feature_map,\n",
        "                 ansatz=ansatz,\n",
        "                 optimizer=COBYLA(maxiter=100),\n",
        "                 quantum_instance=QuantumInstance(BasicAer.get_backend('statevector_simulator'),\n",
        "                                                 seed_simulator=seed,\n",
        "                                                 seed_transpiler=seed)\n",
        "                 )\n",
        "\n",
        "#initialize LGBM classifier\n",
        "clf = lgb.LGBMClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_MTPctWV8f4"
      },
      "source": [
        "###Training the base classifier\n",
        "In the following cells we train the base classifier VQC using 320 training data points, then we predict the labels of both training data and testing data using the trained VQC, the labels we obtained are added as features to initial training and testing data sets"
      ],
      "id": "u_MTPctWV8f4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e8cc882",
      "metadata": {
        "id": "2e8cc882"
      },
      "outputs": [],
      "source": [
        "#train base classifiers and append features to data\n",
        "\n",
        "def level_0():\n",
        "    \n",
        "    #Use VQC and train data, append the predicted labels on train data to train data features \n",
        "    vqc.fit(train_data, train_labels)\n",
        "    a = vqc.predict(train_data)\n",
        "    label_1 = np.delete(a,1,1)\n",
        "    train_added = np.append(train_data,label_1,1)\n",
        "    \n",
        "    #append the predicted labels on test data to test data features\n",
        "    d = vqc.predict(test_data)\n",
        "    label_4 = np.delete(d,1,1)\n",
        "    test_added = np.append(test_data,label_4,1)\n",
        "    \n",
        "    return train_added,test_added"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f17498af",
      "metadata": {
        "id": "f17498af",
        "outputId": "d4b6b17b-26ac-4065-b50f-6e08e54b6aa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training time :  1414.9394099712372\n"
          ]
        }
      ],
      "source": [
        "#get appended train and test data\n",
        "\n",
        "start_time = time()\n",
        "train_added, test_added = level_0()\n",
        "end_time = time()\n",
        "duration = end_time - start_time\n",
        "print(\"training time : \", duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms-ozvtOW5Ir"
      },
      "source": [
        "###Training the meta classifier\n",
        "The LGBM classifier is now trained with training data of 320 data points with 8 features(one additional feature from base classifier VQC). We already have 7 features from original data set, the 8th feature is the prediction label of that particular training data point using the base classifier VQC. It is appended to training data to get 8 features"
      ],
      "id": "ms-ozvtOW5Ir"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0112289c",
      "metadata": {
        "id": "0112289c",
        "outputId": "c330c8db-b659-4f1d-be8a-5410db57e71d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training_data (320, 7)\n",
            "training time :  0.09905099868774414\n"
          ]
        }
      ],
      "source": [
        "#use meta classifier LGBM on appended train data\n",
        "\n",
        "start_time = time()\n",
        "print(\"training_data\", train_data.shape)\n",
        "clf.fit(train_added, train_labels_svm)\n",
        "end_time = time()\n",
        "duration = end_time - start_time\n",
        "print(\"training time : \", duration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY4CBgVPXslx"
      },
      "source": [
        "###Get final prediction\n",
        "We test the appended data set with 8 features (the 8th feature is appended to original 7 features, 8th feature is the prediction label of that particular test data point using trained base classifier) using the trained meta classifier (LGBM) and get our final prediction results"
      ],
      "id": "AY4CBgVPXslx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffbb5b8d",
      "metadata": {
        "id": "ffbb5b8d",
        "outputId": "b15752c2-f298-4b70-e8dc-40781d3ea095"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "testing time :  0.016481876373291016\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.92      0.96     10000\n",
            "           1       0.55      0.97      0.70      1000\n",
            "\n",
            "    accuracy                           0.92     11000\n",
            "   macro avg       0.77      0.94      0.83     11000\n",
            "weighted avg       0.96      0.92      0.93     11000\n",
            "\n",
            "[[9201  799]\n",
            " [  32  968]]\n"
          ]
        }
      ],
      "source": [
        "# predict on appended test data\n",
        "\n",
        "start_time1 = time()\n",
        "y_pred_1 = clf.predict(test_added)\n",
        "end_time1 = time()\n",
        "duration1 = end_time1 - start_time1\n",
        "print(\"testing time : \", duration1)\n",
        "print(classification_report(test_labels_svm,y_pred_1))\n",
        "print(confusion_matrix(test_labels_svm,y_pred_1))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}